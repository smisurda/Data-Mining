# Samantha Misurda
# smisurda@andrew.cmu.edu
# HW4

library(arules)
library(rpart)
library(ROCR)
library(party)

import.csv <- function(filename) {
  return(read.csv(filename, sep = ",", header = TRUE))
}

write.csv <- function(ob, filename) {
  write.table(ob, filename, quote = FALSE, sep = ",", row.names = FALSE)
}

my.data <- import.csv('cars.csv')

#######################################################
# Problem 1
# Entropy, Information gain, Feature selection
#######################################################

# Compute entropy for a vector of symbolic values
# Inputs: Vector of symbolic values
# Output: Entropy (in bits)
entropy <- function(x) {
  #@@@@@@@@ Your function goes here @@@@@@@@@@@@@@@@
  x.levels <- levels(as.factor(x))
  sum <- 0
  
  for(level in x.levels) {
    count <- sum(x == level)
    p <- count / length(x)
    sum <- sum + (-p * log2(p))
  }
  
  return(sum)
}


# Unit test for function entropy
x <- c(rep('A', 3),rep('B', 2),rep('C', 5))
print(ifelse(abs(entropy(x) - 1.485475 ) < 1.0e-05, 'entropy function has passed this test!', 'entropy function has failed this test'))

# Report entropy for cars
entropy(my.data$class)

# Compute information gain IG(x,y)
# Inputs: x, y: vectors of symbolic values of equal length
# Output: information gain IG(x,y)
info.gain <- function(x,y) {

  y.levels <- levels(as.factor(y))
  entropy.x <- entropy(x)
  
  sum <- 0
  
  for(level in y.levels) {
    count <- sum(y == level)
    p <- count / length(y)
    # Have to add back in levels that weren't used
    subset.x <- as.character(x[y==level])
    sum <- sum + p * entropy(subset.x)
  }
  
  return(entropy.x - sum)
}

# Unit test for function info.gain
x <- c(rep('A',3),rep('B',2),rep('C',5))
y <- c(rep('X',4),rep('Y',6))
print(ifelse(abs(info.gain(x,y) - 0.7709506 ) < 1.0e-05, 'Info.gain function has passed this test!', 'info.gain function has failed this test'))

# Report info gain for cars
info.gain(my.data$class, my.data$price)

# Report relative info gain for cars
info.gain(my.data$class, my.data$price)/entropy(my.data$class)


# Information-gain-based feature selection: exhaustive search
# Input: df is a data frame with last column being the output attribute
#        m: size of feature set, default is 1
# Output: data frame with name(s) of selected feature(s), information gain, relative information gain, sorted by the value of information gain
features <- function(df, m = 1){
  nf <- ncol(df) -1 # number of input features
  idx <- 1: nf  # column indices of input features
  output <- df[, ncol(df)]  # output column
  outputH <- entropy(output) # entropy for output
  idx.list <- combn(idx, m) # matrix storing all combinations of size m from idx
  IG.res <-NULL # output data frame
  # iterate through all combinations of index 
  for (ii in 1:ncol(idx.list)){
    this.idx <- idx.list[, ii]  
    input.df <- data.frame(df[,this.idx]) 
    # create a vector where each element is a concatenation of all values of a row of a data frame
    this.input <- apply(input.df, 1, paste, collapse='') 
    # create a new feature name which is a concatenation of feature names in a feature set
    this.input.names <- paste(names(df)[this.idx], collapse=' ')    
    this.IG <-info.gain(this.input,output) # information gain
    this.RIG <- this.IG / outputH # relative information gain
    this.res <- data.frame(feature = this.input.names, IG = this.IG, RIG = this.RIG) #assemble a df
    IG.res <- rbind(IG.res, this.res) # concatenate the results    
  }
  sorted <- IG.res[order(-IG.res$IG), ] # sort the result by information gain in a descending order
  return (sorted)
}

# Problem 1C, i

# Remove the "class" column from the dataset, since it will obviously be the best prediction
my.data.no.predictions <- my.data[,c(1,2,3,4,6,5)]

# Feature set n = 1
feature.set.1 <- features(my.data.no.predictions, 1)
feature.set.1

# Feature set n = 2
feature.set.2 <- features(my.data.no.predictions, 2)
feature.set.2

# Feature set n = 3
feature.set.3 <- features(my.data.no.predictions, 3)
feature.set.3

# Problem 1C, ii

library(ggplot2) # Permitted use in Piazza post

ggplot(data = feature.set.1, aes(x = feature, y = IG)) +
  geom_bar(stat = "identity") + ggtitle("Information Gain for 1 Item Feature Sets") +
  labs(x="Feature",y="IG")


# Plot Relative information gain
ggplot(data = feature.set.1, aes(x = feature, y = RIG)) +
  geom_bar(stat = "identity")+ ggtitle("Relative Information Gain for 1 Item Feature Sets") +
  labs(x="Feature",y="SIG")


##################
# Problem 2
# Association Rules
#################

# Inputs: rules is the object returned by the apriori function
#         df is a data frame from which the rules are learned
# Output: a rule object with extra metrics (95% CI of score)
expand_rule_metrics <- function(rules,df){
  rule.df <- interestMeasure(rules, c('support', 'confidence'), df) # extract metrics into a data frame
  nn <- nrow(df)
  ci.low <-  rule.df$confidence - 1.96 * sqrt( (rule.df$confidence * (1 - rule.df$confidence)) / nn)
  ci.high <- rule.df$confidence + 1.96 * sqrt( (rule.df$confidence * (1 - rule.df$confidence)) / nn)
  quality(rules) <-cbind(quality(rules), ci.low, ci.high) # update the quality slot of the rules object
  return(rules)
}

# Build the set of association rules
#minlen is 2, because 1 creates {} -> A
rules <- apriori(my.data, parameter = list(supp = 0.1, conf = 0.5, minlen = 2, target = "rules"), appearance = list(rhs = c("price=high"),default = "lhs")) 

summary(rules)
inspect(rules)

# Fetch the top 5 for each category
sorted.rules.support <- sort(rules, by = "support")
inspect(sorted.rules.support[1:5])

sorted.rules.confidence <- sort(rules, by="confidence")
inspect(sorted.rules.confidence[1:5])

# Part C, test expand_rule_metrics

expanded.rules <- expand_rule_metrics(rules, my.data)
inspect(expanded.rules)

##################
# Problem 3
#################

# Part A

# Fit and plot a tree using rpart
fit <- rpart(price ~ ., method="class", data = my.data)

printcp(fit) # display the results

# plot tree
plot(fit, uniform = TRUE,
     main="Classification Tree for Price")
text(fit, use.n=TRUE, all=TRUE, cex=.8)

pr <- predict(fit, my.data, type="vector")  
pred <- prediction(pr, my.data$price)

performance(pred, measure = "auc")


# Part B
fit <- ctree(price ~ .,
             data=my.data)

plot(fit, main="Conditional Inference Tree for Price")
print(fit)

pr <- predict(fit, my.data, type="response") 
pred <- prediction(as.numeric(pr)-1, my.data$price)

performance(pred, measure = "auc")
